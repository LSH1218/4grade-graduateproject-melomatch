import os
import sys
import subprocess
import cv2
import pygame
import torch
import random
import json
import numpy as np
import pyaudio
import wave
import warnings
from transformers import AutoTokenizer, Wav2Vec2ForSequenceClassification, AutoModelForSequenceClassification
from mtcnn import MTCNN
from google.cloud import speech
from pydub import AudioSegment
from pygame import mixer
import time
import sys
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from PyQt5.QtGui import *
from PyQt5.QtMultimedia import *
from PyQt5.QtMultimediaWidgets import *
import cv2
import numpy as np
from datetime import datetime
import torchaudio
from deepface import DeepFace
from midiutil import MIDIFile
from googleapiclient.discovery import build
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

from dotenv import load_dotenv
import os

load_dotenv()  # .env íŒŒì¼ ì½ê¸°
API_KEY = os.getenv("API_KEY")
BASE_PATH = os.getenv("BASE_PATH")
WEIGHT_FILE = os.getenv("WEIGHT_FILE", "weights.json")


class KcELECTRAWithDropout(torch.nn.Module):
    def __init__(self):
        super(KcELECTRAWithDropout, self).__init__()
        self.electra = AutoModelForSequenceClassification.from_pretrained("beomi/KcELECTRA-base", num_labels=7)
        self.dropout = torch.nn.Dropout(p=0.6)
        self.batch_norm = torch.nn.BatchNorm1d(7)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        # ElectraForSequenceClassificationì€ token_type_idsë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤
        outputs = self.electra(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )
        logits = self.dropout(outputs.logits)
        return self.batch_norm(logits)

    def load_state_dict(self, state_dict, strict=True):
        try:
            # weights_only=Trueë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë¡œë“œ
            return super().load_state_dict(state_dict, strict=strict)
        except Exception as e:
            print(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise


class EmotionAnalyzer:
    def __init__(self):
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ì œí•œ
            torch.cuda.empty_cache()
            torch.cuda.set_per_process_memory_fraction(0.5)  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ 50%ë¡œ ì œí•œ
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        except:
            print("GPU ì´ˆê¸°í™” ì‹¤íŒ¨, CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.device = torch.device('cpu')

        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì •ì˜
        self.default_weights = {
            "í™”ë‚¨": 1.0,
            "í–‰ë³µ": 1.0,
            "ìŠ¬í””": 1.0,
            "ì¤‘ë¦½": 1.0,
            "ë†€ëŒ": 1.0,
            "ê³µí¬": 1.0,
            "í˜ì˜¤": 1.0
        }
        self.weights = self.load_weights()

        # YouTube API ì´ˆê¸°í™”
        self.youtube = build('youtube', 'v3', developerKey=API_KEY)

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.init_audio_system()
        self.init_models()
        self.init_music_system()

    def init_audio_system(self):
        """ì˜¤ë””ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 44100
        self.chunk = 1024
        self.audio = pyaudio.PyAudio()

    def init_music_system(self):
        """ìŒì•… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.emotion_instruments = {
            "í–‰ë³µ": {"instruments": [0, 24, 73, 40, 105], "tempo": (120, 140), "scale": "major"},
            "ìŠ¬í””": {"instruments": [0, 42, 71, 48, 50], "tempo": (60, 80), "scale": "minor"},
            "í™”ë‚¨": {"instruments": [30, 115, 33, 62, 56], "tempo": (140, 160), "scale": "minor"},
            "ì¤‘ë¦½": {"instruments": [0, 25, 65, 42, 74], "tempo": (90, 110), "scale": "major"},
            "ê³µí¬": {"instruments": [20, 41, 47, 89, 58], "tempo": (100, 120), "scale": "minor"},
            "ë†€ëŒ": {"instruments": [73, 81, 40, 72, 123], "tempo": (110, 130), "scale": "major"},
            "í˜ì˜¤": {"instruments": [43, 34, 58, 47, 55], "tempo": (80, 100), "scale": "minor"}
        }

    def init_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            print(f"Using device: {self.device}")

            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.join(BASE_PATH, "STT.json")
            audio_model_path = os.path.join(BASE_PATH, "ìˆ˜ì •ìŒì„±_model.pt")
            text_model_path = os.path.join(BASE_PATH, "Text_model.pth")

            try:
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore")
                    self.audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(
                        "kresnik/wav2vec2-large-xlsr-korean",
                        num_labels=7,
                        ignore_mismatched_sizes=True
                    ).to('cpu')

                # ì›ë˜ êµ¬ì¡°ëŒ€ë¡œ classifier ìˆ˜ì •
                self.audio_model.classifier = torch.nn.Linear(1024, 7)
                # projectorëŠ” ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •
                self.audio_model.projector = torch.nn.Identity()

                audio_state_dict = torch.load(audio_model_path, map_location='cpu')
                new_state_dict = {k.replace('base_model.', ''): v for k, v in audio_state_dict.items()}

                # classifier ê°€ì¤‘ì¹˜ëŠ” ì œì™¸í•˜ê³  ë¡œë“œ
                classifier_keys = [k for k in new_state_dict.keys() if 'classifier' in k]
                for k in classifier_keys:
                    del new_state_dict[k]

                self.audio_model.load_state_dict(new_state_dict, strict=False)
                self.audio_model.eval()

                if self.device.type == 'cuda':
                    self.audio_model = self.audio_model.to(self.device)
            except RuntimeError as e:
                print(f"ì˜¤ë””ì˜¤ ëª¨ë¸ GPU ë¡œë“œ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
                self.device = torch.device('cpu')

            try:
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore")
                    # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
                    self.tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")

                    # í…ìŠ¤íŠ¸ ëª¨ë¸ ì´ˆê¸°í™”
                    self.text_model = KcELECTRAWithDropout().to('cpu')
                    # weights_only ì˜µì…˜ ì œê±°
                    text_state_dict = torch.load(text_model_path, map_location='cpu')
                    self.text_model.load_state_dict(text_state_dict, strict=True)
                    self.text_model.eval()

                    if self.device.type == 'cuda':
                        self.text_model = self.text_model.to(self.device)
            except RuntimeError as e:
                print(f"í…ìŠ¤íŠ¸ ëª¨ë¸ GPU ë¡œë“œ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
                self.device = torch.device('cpu')
                if hasattr(self, 'text_model'):
                    self.text_model = self.text_model.to('cpu')

            # MTCNN ì–¼êµ´ ê°ì§€ê¸° ì´ˆê¸°í™”
            try:
                self.detector = MTCNN()
            except Exception as e:
                print(f"MTCNN ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise

        except Exception as e:
            print(f"ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def capture_image_until_face(self):
        """ì–¼êµ´ì´ ì¸ì‹ë  ë•Œê¹Œì§€ ì›¹ìº  í™œì„±í™”í•˜ê³  ì •ë³´ í‘œì‹œ"""
        print("\nì›¹ìº ì„ í™œì„±í™”í•©ë‹ˆë‹¤. ì–¼êµ´ì´ ì¸ì‹ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤...")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ëˆ„ë¥´ì„¸ìš”")

        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return None

        face_detected = False
        detected_frame = None
        face_info = None

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    continue

                # ì¡°ë„ ì¡°ì • - íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™” ì ìš©
                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                equalized_frame = cv2.equalizeHist(gray_frame)
                display_frame = cv2.cvtColor(equalized_frame, cv2.COLOR_GRAY2BGR)

                # ì–¼êµ´ ê°ì§€
                faces = self.detector.detect_faces(display_frame)

                if faces and not face_detected:
                    print("ì–¼êµ´ì´ ì¸ì‹ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    face_detected = True
                    detected_frame = display_frame.copy()

                    # DeepFace ë¶„ì„
                    face = faces[0]
                    x, y, w, h = face['box']
                    face_frame = frame[y:y + h, x:x + w]

                    try:
                        analysis = DeepFace.analyze(
                            face_frame,
                            actions=['age', 'gender'],
                            enforce_detection=False,
                            silent=True
                        )

                        if isinstance(analysis, list):
                            analysis = analysis[0]

                        # ì„±ë³„ íŒë‹¨: ì„ê³„ê°’ ì„¤ì •ìœ¼ë¡œ ë³´ìˆ˜ì  íŒë‹¨
                        male_confidence = analysis['gender']['Man']
                        female_confidence = analysis['gender']['Woman']

                        if abs(male_confidence - female_confidence) < 10:  # ì„±ë³„ êµ¬ë¶„ì´ ì• ë§¤í•œ ê²½ìš° 'Unknown'ìœ¼ë¡œ ì„¤ì •
                            gender = 'Unknown'
                        else:
                            gender = 'Male' if male_confidence > female_confidence else 'Female'

                        age_group = self.classify_age(analysis['age'])
                        face_info = (gender, age_group)
                    except Exception as e:
                        print(f"ì–¼êµ´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                # ì–¼êµ´ ì •ë³´ í‘œì‹œ
                if face_info:
                    gender, age_group = face_info
                    cv2.putText(display_frame, f"Gender: {gender}", (10, 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    cv2.putText(display_frame, f"Age Group: {age_group}", (10, 70),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

                # ì–¼êµ´ ì¸ì‹ í‘œì‹œ
                if faces:
                    for face in faces:
                        x, y, w, h = face['box']
                        cv2.rectangle(display_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

                cv2.imshow('Webcam', display_frame)

                if face_detected and detected_frame is not None:
                    return detected_frame

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

            return None
        finally:
            cap.release()
            cv2.destroyAllWindows()

    def record_audio_stream(self):
        """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘"""
        print("\në§ˆì´í¬ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤. ë§ì”€í•˜ì‹¤ ë•Œ ë…¹ìŒì´ ì‹œì‘ë©ë‹ˆë‹¤...")
        try:
            stream = self.audio.open(
                format=self.audio_format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )
            frames = []
            print("ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤...")

            for _ in range(0, int(self.rate / self.chunk * 5)):  # 5ì´ˆê°„ ë…¹ìŒ
                data = stream.read(self.chunk)
                frames.append(data)

            print("ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # íŒŒì¼ ì €ì¥
            with wave.open('recorded_audio.wav', 'wb') as wf:
                wf.setnchannels(self.channels)
                wf.setsampwidth(self.audio.get_sample_size(self.audio_format))
                wf.setframerate(self.rate)
                wf.writeframes(b''.join(frames))

            return 'recorded_audio.wav'
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
        finally:
            if 'stream' in locals():
                stream.stop_stream()
                stream.close()

    def analyze_emotions(self):
        """ê°ì • ë¶„ì„ ì‹¤í–‰"""
        frame = self.capture_image_until_face()
        if frame is None:
            return None

        try:
            faces = self.detector.detect_faces(frame)
            face = faces[0]
            x, y, w, h = face['box']
            face_frame = frame[y:y + h, x:x + w]

            face_analysis = DeepFace.analyze(face_frame,
                                             actions=['age', 'gender'],
                                             enforce_detection=False,
                                             silent=True)

            if isinstance(face_analysis, list):
                face_analysis = face_analysis[0]

            print("\n=== ì–¼êµ´ ë¶„ì„ ê²°ê³¼ ===")
            print(f"ê°ì§€ëœ ì„±ë³„: {face_analysis['gender']}")
            print(f"ì¶”ì • ë‚˜ì‡ëŒ€: {face_analysis['age']}")

            audio_file = self.record_audio_stream()
            if audio_file is None:
                return None

            emotion = self.analyze_audio_emotion(audio_file)
            return {
                'gender': 'Male' if face_analysis['gender']['Man'] > face_analysis['gender']['Woman'] else 'Female',
                'age_group': self.classify_age(face_analysis['age']),
                'emotion': emotion
            }
        except Exception as e:
            print(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def analyze_audio_emotion(self, audio_path):
        """ì˜¤ë””ì˜¤ ê°ì • ë¶„ì„"""
        print("\nìŒì„±ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        try:
            with torch.no_grad():
                # ì˜¤ë””ì˜¤ ì²˜ë¦¬
                waveform, sample_rate = torchaudio.load(audio_path)

                # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)

                # ìƒ˜í”Œë ˆì´íŠ¸ ë³€í™˜
                resampler = torchaudio.transforms.Resample(
                    orig_freq=sample_rate,
                    new_freq=16000
                )
                waveform = resampler(waveform)

                # ë°ì´í„° ì •ê·œí™” ë° í¬ê¸° ì¡°ì •
                max_length = 16000 * 10  # 10ì´ˆ ê¸¸ì´ë¡œ ì œí•œ
                if waveform.shape[-1] > max_length:
                    waveform = waveform[..., :max_length]

                # íŒ¨ë”© ì¶”ê°€
                if waveform.shape[-1] < max_length:
                    padding_length = max_length - waveform.shape[-1]
                    waveform = torch.nn.functional.pad(waveform, (0, padding_length))

                # CPUì—ì„œ ì²˜ë¦¬ í›„ GPUë¡œ ì´ë™
                waveform = waveform.to('cpu')
                waveform = waveform.float()  # ëª…ì‹œì ìœ¼ë¡œ float32ë¡œ ë³€í™˜

                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
                if len(waveform.shape) == 1:
                    waveform = waveform.unsqueeze(0)
                if len(waveform.shape) == 2:
                    waveform = waveform.unsqueeze(0)

                # ì´ì œ GPUë¡œ ì´ë™
                waveform = waveform.to(self.device)

                try:
                    # ë” ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬
                    audio_emotion = self.audio_model(waveform).logits
                except RuntimeError as e:
                    print("GPU ì²˜ë¦¬ ì‹¤íŒ¨, CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                    # GPU ì‹¤íŒ¨ì‹œ CPUë¡œ í´ë°±
                    waveform = waveform.cpu()
                    self.audio_model = self.audio_model.cpu()
                    audio_emotion = self.audio_model(waveform).logits
                    self.audio_model = self.audio_model.to(self.device)

                # STT ë³€í™˜
                client = speech.SpeechClient()
                with open(audio_path, "rb") as audio_file:
                    content = audio_file.read()

                response = client.recognize(
                    config=speech.RecognitionConfig(
                        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                        language_code="ko-KR"
                    ),
                    audio=speech.RecognitionAudio(content=content)
                )

                transcript = ""
                for result in response.results:
                    transcript = result.alternatives[0].transcript

                # í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
                text_input = self.tokenizer(transcript, padding=True, truncation=True, return_tensors="pt")
                text_input = {k: v.to(self.device) for k, v in text_input.items()}

                try:
                    text_emotion = self.text_model(**text_input)
                except RuntimeError:
                    print("GPU ì²˜ë¦¬ ì‹¤íŒ¨, CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
                    # GPU ì‹¤íŒ¨ì‹œ CPUë¡œ í´ë°±
                    text_input = {k: v.cpu() for k, v in text_input.items()}
                    self.text_model = self.text_model.cpu()
                    text_emotion = self.text_model(**text_input)
                    self.text_model = self.text_model.to(self.device)

                # ê²°ê³¼ ê²°í•©
                combined_emotion = 0.9 * text_emotion + 0.1 * audio_emotion
                emotion_idx = torch.argmax(torch.softmax(combined_emotion, dim=1)).item()

                emotions = ["í™”ë‚¨", "í–‰ë³µ", "ìŠ¬í””", "ì¤‘ë¦½", "ë†€ëŒ", "ê³µí¬", "í˜ì˜¤"]
                return emotions[emotion_idx]

        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # CPUë¡œ ì „í™˜ ì‹œë„
            try:
                self.device = torch.device('cpu')
                self.audio_model = self.audio_model.cpu()
                self.text_model = self.text_model.cpu()
                return self.analyze_audio_emotion(audio_path)
            except Exception as e:
                print(f"CPU ì²˜ë¦¬ ì¤‘ì—ë„ ì˜¤ë¥˜ ë°œìƒ: {e}")
                raise

    def create_music(self, emotion):
        """í–¥ìƒëœ MIDI ìŒì•… ìƒì„± - ë” ê¸´ ë²„ì „"""
        print(f"\n{emotion} ê°ì •ì— ë§ëŠ” ìŒì•…ì„ ì‘ê³¡í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        try:
            emotion_config = self.emotion_instruments[emotion]
            tempo = random.randint(*emotion_config["tempo"])
            scale_type = emotion_config["scale"]

            # íŠ¸ë™ ìˆ˜ ì„¤ì •
            track_count = len(emotion_config["instruments"])
            midi = MIDIFile(track_count)

            # ìŒì•… êµ¬ì¡° ì •ì˜ - ë” ê¸´ ê¸¸ì´ë¡œ ìˆ˜ì •
            measures = 32  # ë§ˆë”” ìˆ˜ë¥¼ 32ë§ˆë””ë¡œ ì¦ê°€
            beats_per_measure = 4  # 4/4 ë°•ì
            total_beats = measures * beats_per_measure

            # ìŒê³„ ì •ì˜
            scales = {
                "major": [0, 2, 4, 5, 7, 9, 11],  # C major scale
                "minor": [0, 2, 3, 5, 7, 8, 10]  # C minor scale
            }
            current_scale = scales[scale_type]

            # ì„¹ì…˜ êµ¬ì¡° ì •ì˜ (A-B-A í˜•ì‹)
            section_a = total_beats // 2  # ì „ì²´ì˜ ì ˆë°˜
            section_b = total_beats // 4  # ì „ì²´ì˜ 1/4

            for track in range(track_count):
                # íŠ¸ë™ ê¸°ë³¸ ì„¤ì •
                midi.addTempo(track, 0, tempo)
                midi.addProgramChange(track, track, 0, emotion_config["instruments"][track])

                # íŠ¸ë™ë³„ íŠ¹ì„± ì„¤ì •
                if track == 0:  # ë©”ì¸ ë©œë¡œë””
                    volume_range = (90, 100)
                    note_durations = [1, 0.5, 0.25, 2]  # ë‹¤ì–‘í•œ ìŒí‘œ ê¸¸ì´
                    octave_range = (5, 6)
                else:  # ë°˜ì£¼
                    volume_range = (60, 80)
                    note_durations = [0.5, 1, 2, 4]
                    octave_range = (3, 5)

                # A ì„¹ì…˜ ìƒì„±
                time = 0
                melody_a = []  # A ì„¹ì…˜ì˜ ë©œë¡œë”” ì €ì¥
                while time < section_a:
                    duration = random.choice(note_durations)
                    if time + duration > section_a:
                        duration = section_a - time

                    note = random.choice(current_scale)
                    octave = random.randint(*octave_range)
                    pitch = note + (12 * octave)
                    volume = random.randint(*volume_range)

                    # ë©œë¡œë”” ì €ì¥ ë° ì¶”ê°€
                    melody_a.append((pitch, duration, volume))
                    midi.addNote(track, track, pitch, time, duration, volume)

                    # í™”ìŒ ì¶”ê°€ (ë©”ì¸ ë©œë¡œë””ì˜ ê²½ìš°)
                    if track == 0 and random.random() < 0.3:
                        harmony_note = (note + 4) % 12
                        harmony_pitch = harmony_note + (12 * octave)
                        midi.addNote(track, track, harmony_pitch, time, duration, volume - 20)

                    time += duration

                # B ì„¹ì…˜ ìƒì„± (ë³€ì£¼)
                time = section_a
                while time < section_a + section_b:
                    duration = random.choice(note_durations)
                    if time + duration > section_a + section_b:
                        duration = section_a + section_b - time

                    note = random.choice(current_scale)
                    octave = random.randint(*octave_range)
                    pitch = note + (12 * octave)
                    volume = random.randint(*volume_range)

                    midi.addNote(track, track, pitch, time, duration, volume)
                    time += duration

                # A ì„¹ì…˜ ë°˜ë³µ (ì²˜ìŒ ì €ì¥í•œ ë©œë¡œë”” ì¬ì‚¬ìš©)
                time = section_a + section_b
                for pitch, duration, volume in melody_a:
                    if time + duration > total_beats:
                        break
                    midi.addNote(track, track, pitch, time, duration, volume)
                    time += duration

            # íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"generated_{emotion}_{timestamp}.mid"

            with open(filename, "wb") as output_file:
                midi.writeFile(output_file)

            print(f"MIDI íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
            return filename

        except Exception as e:
            print(f"ìŒì•… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def play_midi_file(self, midi_file):
        """í–¥ìƒëœ MIDI íŒŒì¼ ì¬ìƒ"""
        try:
            # pygame ì™„ì „íˆ ì´ˆê¸°í™”
            pygame.init()
            pygame.mixer.quit()  # ê¸°ì¡´ mixer ì´ˆê¸°í™”
            pygame.mixer.init(frequency=44100)

            if not os.path.exists(midi_file):
                print(f"MIDI íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {midi_file}")
                return True

            try:
                # ì¬ìƒ ë³¼ë¥¨ ì„¤ì •
                pygame.mixer.music.set_volume(0.7)
                pygame.mixer.music.load(midi_file)
                pygame.mixer.music.play()

                print("\n=== MIDI íŒŒì¼ ì¬ìƒ ì¤‘ ===")
                print("H: ì¬ìƒ ì¤‘ì§€")
                print("P: í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
                print("R: ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°")
                print("â†‘: ë³¼ë¥¨ ì¦ê°€")
                print("â†“: ë³¼ë¥¨ ê°ì†Œ")
                print("Space: ì¼ì‹œì •ì§€/ì¬ìƒ")

                paused = False
                while pygame.mixer.music.get_busy() or paused:
                    key = cv2.waitKey(1) & 0xFF

                    # ì¬ìƒ ì œì–´
                    if key == ord('h'):  # ì¤‘ì§€
                        pygame.mixer.music.stop()
                        break
                    elif key == ord('p'):  # í”„ë¡œê·¸ë¨ ì¢…ë£Œ
                        pygame.mixer.music.stop()
                        return False
                    elif key == ord('r'):  # ì²˜ìŒìœ¼ë¡œ
                        pygame.mixer.music.stop()
                        break
                    elif key == 32:  # Space - ì¼ì‹œì •ì§€/ì¬ìƒ
                        if paused:
                            pygame.mixer.music.unpause()
                            print("ì¬ìƒ ì¬ê°œ")
                        else:
                            pygame.mixer.music.pause()
                            print("ì¼ì‹œì •ì§€")
                        paused = not paused

                    # ë³¼ë¥¨ ì œì–´
                    elif key == 82:  # Up arrow - ë³¼ë¥¨ ì¦ê°€
                        current_volume = pygame.mixer.music.get_volume()
                        pygame.mixer.music.set_volume(min(1.0, current_volume + 0.1))
                        print(f"ë³¼ë¥¨: {pygame.mixer.music.get_volume():.1f}")
                    elif key == 84:  # Down arrow - ë³¼ë¥¨ ê°ì†Œ
                        current_volume = pygame.mixer.music.get_volume()
                        pygame.mixer.music.set_volume(max(0.0, current_volume - 0.1))
                        print(f"ë³¼ë¥¨: {pygame.mixer.music.get_volume():.1f}")

                    pygame.time.wait(100)

                return True

            except pygame.error as e:
                print(f"MIDI ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return True

        except Exception as e:
            print(f"MIDI íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return True

        finally:
            try:
                pygame.mixer.quit()
                pygame.quit()
            except:
                pass

    def get_music_recommendations(self, gender, age_group, emotion):
        """ìŒì•… ì¶”ì²œ"""
        print("\nì‚¬ìš©ì ë§ì¶¤ ìŒì•…ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        emotion_keywords = {
            "í™”ë‚¨": "calming relaxing",
            "í–‰ë³µ": "upbeat happy",
            "ìŠ¬í””": "comforting healing",
            "ì¤‘ë¦½": "ambient peaceful",
            "ë†€ëŒ": "soothing meditation",
            "ê³µí¬": "peaceful calming",
            "í˜ì˜¤": "positive uplifting"
        }

        age_preferences = {
            "0-10": "children songs",
            "11-20": "pop kpop",
            "21-39": "indie alternative",
            "40-59": "classical jazz",
            "60+": "traditional healing"
        }

        try:
            base_query = f"{emotion_keywords[emotion]} music {age_preferences[age_group]}"
            request = self.youtube.search().list(
                part="snippet",
                maxResults=10,
                q=base_query,
                type="video"
            )

            response = request.execute()
            videos = response['items']
            weighted_choice = random.choices(
                videos,
                weights=[self.weights[emotion]] * len(videos),
                k=1
            )[0]

            return {
                'title': weighted_choice['snippet']['title'],
                'url': f"https://www.youtube.com/watch?v={weighted_choice['id']['videoId']}"
            }
        except Exception as e:
            print(f"YouTube ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def play_midi_file(self, midi_file):
        """MIDI íŒŒì¼ ì¬ìƒ"""
        try:
            # pygame ì´ˆê¸°í™”
            mixer.init()
            mixer.music.load(midi_file)
            mixer.music.play()

            print("\n=== MIDI íŒŒì¼ ì¬ìƒ ì¤‘ ===")
            print("H: ì¬ìƒ ì¤‘ì§€")
            print("P: í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
            print("R: ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°")

            while mixer.music.get_busy():  # ìŒì•…ì´ ì¬ìƒ ì¤‘ì¼ ë•Œë§Œ ë°˜ë³µ
                key = cv2.waitKey(1) & 0xFF
                if key == ord('h'):  # H key
                    mixer.music.stop()
                    break
                elif key == ord('p'):  # P key
                    mixer.music.stop()
                    return False
                elif key == ord('r'):  # R key
                    mixer.music.stop()
                    break
                time.sleep(0.1)

            mixer.quit()
            return True

        except Exception as e:
            print(f"MIDI íŒŒì¼ ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if mixer.get_init():
                mixer.quit()
            return True

    def play_youtube_video(self, video_url):
        """YouTube ë¹„ë””ì˜¤ ì¬ìƒ"""
        try:
            options = webdriver.ChromeOptions()
            options.add_argument('--start-maximized')
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_experimental_option('excludeSwitches', ['enable-automation'])
            options.add_experimental_option('useAutomationExtension', False)

            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)

            print(f"YouTube URL ì—´ê¸°: {video_url}")
            driver.get(video_url)

            wait = WebDriverWait(driver, 20)

            try:
                skip_button = wait.until(
                    EC.presence_of_element_located((By.CLASS_NAME, "ytp-ad-skip-button"))
                )
                skip_button.click()
            except:
                pass

            try:
                play_button = wait.until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, ".ytp-large-play-button"))
                )
                driver.execute_script("arguments[0].click();", play_button)
            except:
                print("ìë™ ì¬ìƒ ì‹¤íŒ¨, ìˆ˜ë™ìœ¼ë¡œ ì¬ìƒí•´ì£¼ì„¸ìš”.")

            # í‚¤ ì„¤ì • ë³€ê²½
            print("\n=== ì¬ìƒ ì œì–´ ===")
            print("H: ì¬ìƒ ì¤‘ì§€")
            print("P: í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
            print("R: ìŒì•… ì¤‘ì§€ í›„ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°")

            while True:
                key = cv2.waitKey(1) & 0xFF
                if key == ord('h'):  # H key
                    break
                elif key == ord('p'):  # P key
                    return None  # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹ í˜¸
                elif key == ord('r'):  # R key
                    break

            return driver

        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if 'driver' in locals():
                driver.quit()
            return None

    def load_weights(self):
        """ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        try:
            if os.path.exists(WEIGHT_FILE):
                with open(WEIGHT_FILE, 'r') as file:
                    return json.load(file)
            return self.default_weights.copy()
        except Exception as e:
            print(f"ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self.default_weights.copy()

    def save_weights(self):
        """ê°€ì¤‘ì¹˜ ì €ì¥"""
        try:
            with open(WEIGHT_FILE, 'w') as file:
                json.dump(self.weights, file, indent=4)
        except Exception as e:
            print(f"ê°€ì¤‘ì¹˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def update_weights(self, emotion, liked=True):
        """ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        try:
            if liked:
                self.weights[emotion] = round(self.weights[emotion] + 0.1, 2)
            else:
                self.weights[emotion] = max(0.1, round(self.weights[emotion] - 0.1, 2))
            self.save_weights()
        except Exception as e:
            print(f"ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    @staticmethod
    def classify_age(age):
        """ë‚˜ì´ ê·¸ë£¹ ë¶„ë¥˜"""
        if age <= 10:
            return "0-10"
        elif age <= 20:
            return "11-20"
        elif age <= 39:
            return "21-39"
        elif age <= 59:
            return "40-59"
        else:
            return "60+"

    def play_youtube_video(self, video_url):
        """YouTube ë¹„ë””ì˜¤ ì¬ìƒ"""
        driver = None
        try:
            service = Service(ChromeDriverManager().install())
            options = webdriver.ChromeOptions()
            driver = webdriver.Chrome(service=service, options=options)
            driver.get(video_url)

            play_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "button.ytp-large-play-button.ytp-button"))
            )
            play_button.click()

            return driver
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if driver:
                driver.quit()
            return None

    def analyze_emotions(self):
        """ê°ì • ë¶„ì„ ì‹¤í–‰"""
        # ì›¹ìº ì—ì„œ ì–¼êµ´ ì¸ì‹
        frame = self.capture_image_until_face()
        if frame is None:
            return None

        try:
            # ì–¼êµ´ ë¶„ì„
            faces = self.detector.detect_faces(frame)
            face = faces[0]  # ì´ë¯¸ ì–¼êµ´ì´ ì¸ì‹ëœ ìƒíƒœ
            x, y, w, h = face['box']
            face_frame = frame[y:y + h, x:x + w]

            # DeepFace ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ì¶”ê°€
            face_analysis = DeepFace.analyze(face_frame,
                                             actions=['age', 'gender'],
                                             enforce_detection=False,
                                             silent=True)  # ì§„í–‰ë¥  í‘œì‹œì¤„ ìˆ¨ê¸°ê¸°

            print("\n=== ì–¼êµ´ ë¶„ì„ ê²°ê³¼ ===")
            if isinstance(face_analysis, list):
                face_analysis = face_analysis[0]
            print(f"ê°ì§€ëœ ì„±ë³„: {face_analysis['gender']}")
            print(f"ì¶”ì • ë‚˜ì‡ëŒ€: {face_analysis['age']}")

        except Exception as e:
            print(f"ì–¼êµ´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

        # ìŒì„± ë…¹ìŒ ë° ë¶„ì„
        audio_file = self.record_audio_stream()
        if audio_file is None:
            return None

        try:
            emotion = self.analyze_audio_emotion(audio_file)
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

        return {
            'gender': 'Male' if face_analysis['gender']['Man'] > face_analysis['gender']['Woman'] else 'Female',
            'age_group': self.classify_age(face_analysis['age']),
            'emotion': emotion
        }

    def analyze_audio_emotion(self, audio_path):
        """ì˜¤ë””ì˜¤ ê°ì • ë¶„ì„"""
        print("\nìŒì„±ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        try:
            with torch.no_grad():
                # ì˜¤ë””ì˜¤ ì²˜ë¦¬
                waveform, sample_rate = torchaudio.load(audio_path)

                # ìŠ¤í…Œë ˆì˜¤ë¥¼ ëª¨ë…¸ë¡œ ë³€í™˜
                if waveform.shape[0] > 1:
                    waveform = torch.mean(waveform, dim=0, keepdim=True)

                # ìƒ˜í”Œë ˆì´íŠ¸ ë³€í™˜
                resampler = torchaudio.transforms.Resample(
                    orig_freq=sample_rate,
                    new_freq=16000
                )
                waveform = resampler(waveform)

                # ì°¨ì› ì¡°ì •
                waveform = waveform.squeeze(0)  # Remove extra dimension
                if len(waveform.shape) == 1:
                    waveform = waveform.unsqueeze(0)

                # ì¥ì¹˜ ì´ë™
                waveform = waveform.to(self.device)

                # ê°ì • ë¶„ì„
                audio_emotion = self.audio_model(waveform).logits

                # STT ë³€í™˜
                client = speech.SpeechClient()
                with open(audio_path, "rb") as audio_file:
                    content = audio_file.read()

                response = client.recognize(
                    config=speech.RecognitionConfig(
                        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                        language_code="ko-KR"
                    ),
                    audio=speech.RecognitionAudio(content=content)
                )

                transcript = ""
                for result in response.results:
                    transcript = result.alternatives[0].transcript

                # í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
                text_input = self.tokenizer(transcript, padding=True, truncation=True, return_tensors="pt")
                text_input = {k: v.to(self.device) for k, v in text_input.items()}
                text_emotion = self.text_model(**text_input)

                # ê²°ê³¼ ê²°í•©
                combined_emotion = 0.9 * text_emotion + 0.1 * audio_emotion
                emotion_idx = torch.argmax(torch.softmax(combined_emotion, dim=1)).item()

                emotions = ["í™”ë‚¨", "í–‰ë³µ", "ìŠ¬í””", "ì¤‘ë¦½", "ë†€ëŒ", "ê³µí¬", "í˜ì˜¤"]
                return emotions[emotion_idx]

        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    def create_music(self, emotion):
        """AI ì‘ê³¡"""
        print(f"\n{emotion} ê°ì •ì— ë§ëŠ” ìŒì•…ì„ ì‘ê³¡í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        emotion_config = self.emotion_instruments[emotion.lower()]
        tempo = random.randint(*emotion_config["tempo"])

        midi = MIDIFile(len(emotion_config["instruments"]))

        for track, instrument in enumerate(emotion_config["instruments"]):
            midi.addTempo(track, 0, tempo)
            midi.addProgramChange(track, track, 0, instrument)

            # ê¸°ë³¸ ë©œë¡œë”” ìƒì„±
            base_note = 60
            for i in range(16):
                note = base_note + random.randint(-5, 5)
                midi.addNote(track, track, note, i, 1, random.randint(60, 100))

        filename = f"generated_{emotion}.mid"
        with open(filename, "wb") as output_file:
            midi.writeFile(output_file)
        return filename

    def get_music_recommendations(self, gender, age_group, emotion):
        """ìŒì•… ì¶”ì²œ"""
        print("\nì‚¬ìš©ì ë§ì¶¤ ìŒì•…ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        emotion_keywords = {
            "í™”ë‚¨": "calming relaxing",
            "í–‰ë³µ": "upbeat happy",
            "ìŠ¬í””": "comforting healing",
            "ì¤‘ë¦½": "ambient peaceful",
            "ë†€ëŒ": "soothing meditation",
            "ê³µí¬": "peaceful calming",
            "í˜ì˜¤": "positive uplifting"
        }

        age_preferences = {
            "0-10": "children songs",
            "11-20": "pop kpop",
            "21-39": "indie alternative",
            "40-59": "classical jazz",
            "60+": "traditional healing"
        }

        base_query = f"{emotion_keywords[emotion]} music {age_preferences[age_group]}"

        try:
            request = self.youtube.search().list(
                part="snippet",
                maxResults=10,
                q=base_query,
                type="video"
            )

            response = request.execute()
            videos = response['items']
            weighted_choice = random.choices(
                videos,
                weights=[self.weights[emotion]] * len(videos),
                k=1
            )[0]

            return {
                'title': weighted_choice['snippet']['title'],
                'url': f"https://www.youtube.com/watch?v={weighted_choice['id']['videoId']}"
            }
        except Exception as e:
            print(f"YouTube ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None


def play_youtube_video(self, video_url):
    """YouTube ë¹„ë””ì˜¤ ì¬ìƒ"""
    try:
        service = Service(ChromeDriverManager().install())
        options = webdriver.ChromeOptions()
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(video_url)

        play_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button.ytp-large-play-button.ytp-button"))
        )
        play_button.click()

        return driver
    except Exception as e:
        print(f"ë¹„ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def load_weights(self):
    """ê°€ì¤‘ì¹˜ ë¡œë“œ"""
    if os.path.exists(WEIGHT_FILE):
        with open(WEIGHT_FILE, 'r') as file:
            return json.load(file)
    return self.default_weights.copy()

    def save_weights(self):
        """ê°€ì¤‘ì¹˜ ì €ì¥"""
        try:
            with open(WEIGHT_FILE, 'w') as file:
                json.dump(self.weights, file, indent=4)
        except Exception as e:
            print(f"ê°€ì¤‘ì¹˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def update_weights(self, emotion, liked=True):
        """ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        try:
            if liked:
                self.weights[emotion] = round(self.weights[emotion] + 0.1, 2)
            else:
                self.weights[emotion] = max(0.1, round(self.weights[emotion] - 0.1, 2))
            self.save_weights()
        except Exception as e:
            print(f"ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    @staticmethod
    def classify_age(age):
        """ë‚˜ì´ ê·¸ë£¹ ë¶„ë¥˜"""
        if age <= 10:
            return "0-10"
        elif age <= 20:
            return "11-20"
        elif age <= 39:
            return "21-39"
        elif age <= 59:
            return "40-59"
        else:
            return "60+"


class WebcamWidget(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.video_size = QSize(640, 480)
        self.setup_ui()
        self.setup_camera()

    def stop_camera(self):
        if hasattr(self, 'capture'):
            self.timer.stop()
            self.capture.release()
            self.capture = None

    def start_camera(self):
        if not hasattr(self, 'capture') or self.capture is None:
            self.setup_camera()

    def setup_ui(self):
        self.image_label = QLabel()
        self.image_label.setFixedSize(self.video_size)

        self.capture_button = QPushButton("ê°ì • ë¶„ì„ ì‹œì‘")
        self.capture_button.setStyleSheet("""
            QPushButton {
                background-color: #007AFF;
                color: white;
                border-radius: 5px;
                padding: 10px;
                font-size: 14px;
            }
            QPushButton:hover {
                background-color: #0056b3;
            }
        """)

        layout = QVBoxLayout()
        layout.addWidget(self.image_label)
        layout.addWidget(self.capture_button)
        layout.setAlignment(Qt.AlignCenter)
        self.setLayout(layout)

    def setup_camera(self):
        self.capture = cv2.VideoCapture(0)
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.timer.start(30)

    def update_frame(self):
        ret, frame = self.capture.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = frame.shape
            bytes_per_line = ch * w
            qt_image = QImage(frame.data, w, h, bytes_per_line, QImage.Format_RGB888)
            scaled_image = qt_image.scaled(self.video_size, Qt.KeepAspectRatio)
            self.image_label.setPixmap(QPixmap.fromImage(scaled_image))


class ResultWidget(QWidget):
    def __init__(self, emotion_data, parent=None):
        super().__init__(parent)
        self.emotion_data = emotion_data
        self.setup_ui()
        self.is_playing = False

    def setup_ui(self):
        layout = QVBoxLayout()

        # íƒ€ì´í‹€
        title = QLabel("ì˜¤ëŠ˜ì˜ ê°ì • ë¶„ì„ ê²°ê³¼")
        title.setStyleSheet("""
            QLabel {
                font-size: 24px;
                font-weight: bold;
                margin: 20px;
            }
        """)
        title.setAlignment(Qt.AlignCenter)
        layout.addWidget(title)

        # ê°ì • ê²°ê³¼
        self.emotion_label = QLabel()
        self.emotion_label.setStyleSheet("""
            QLabel {
                font-size: 48px;
                margin: 20px;
            }
        """)
        self.emotion_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.emotion_label)

        # ê°ì • ì„¤ëª…
        self.description_label = QLabel()
        self.description_label.setStyleSheet("""
            QLabel {
                font-size: 16px;
                margin: 20px;
            }
        """)
        self.description_label.setAlignment(Qt.AlignCenter)
        self.description_label.setWordWrap(True)
        layout.addWidget(self.description_label)

        # ë¶„ì„ ê²°ê³¼ ìƒì„¸
        self.details_label = QLabel()
        self.details_label.setStyleSheet("""
            QLabel {
                font-size: 14px;
                margin: 20px;
            }
        """)
        self.details_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.details_label)

        # ìŒì•… ì„ íƒ ë²„íŠ¼
        music_group = QGroupBox("ìŒì•… ì„ íƒ")
        music_layout = QHBoxLayout()

        self.ai_music_button = QPushButton("AI ì‘ê³¡ ìŒì•…")
        self.youtube_music_button = QPushButton("YouTube ìŒì•…")

        for button in [self.ai_music_button, self.youtube_music_button]:
            button.setStyleSheet("""
                QPushButton {
                    background-color: #007AFF;
                    color: white;
                    border-radius: 5px;
                    padding: 10px;
                    font-size: 14px;
                    min-width: 150px;
                }
                QPushButton:hover {
                    background-color: #0056b3;
                }
                QPushButton:disabled {
                    background-color: #cccccc;
                }
            """)
            music_layout.addWidget(button)

        music_group.setLayout(music_layout)
        layout.addWidget(music_group)

        # YouTube í”Œë ˆì´ì–´ ì˜ì—­
        self.youtube_widget = QWidget()
        self.youtube_widget.hide()
        layout.addWidget(self.youtube_widget)

        # í•˜ë‹¨ ë²„íŠ¼
        button_layout = QHBoxLayout()
        self.new_button = QPushButton("ìƒˆë¡œìš´ ì¼ê¸° ì“°ê¸°")
        self.new_button.setStyleSheet("""
            QPushButton {
                background-color: #6c757d;
                color: white;
                border-radius: 5px;
                padding: 10px;
                font-size: 14px;
                min-width: 150px;
            }
            QPushButton:hover {
                background-color: #5a6268;
            }
        """)
        button_layout.addWidget(self.new_button)
        layout.addLayout(button_layout)
        self.setLayout(layout)

    def update_result(self, gender, age_group, emotion):
        emoji = self.emotion_data[emotion]['emoji']
        description = self.emotion_data[emotion]['description']

        self.emotion_label.setText(f"{emoji}\nì˜¤ëŠ˜ì˜ ê°ì •: {emotion}")
        self.description_label.setText(description)
        self.details_label.setText(f"ì„±ë³„: {gender}\në‚˜ì´ëŒ€: {age_group}\nê°ì •: {emotion}")


class EmotionAnalyzerGUI(QMainWindow):
    def __init__(self):
        super().__init__()
        self.emotion_data = {
            'í™”ë‚¨': {'emoji': 'ğŸ˜ ', 'description': 'í™”ë‚œ ê°ì •ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ ì‹¬í˜¸í¡ì„ í•˜ê³  ë§ˆìŒì„ ì§„ì •ì‹œì¼œë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?'},
            'í–‰ë³µ': {'emoji': 'ğŸ˜Š', 'description': 'í–‰ë³µí•œ ê°ì •ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê¸°ë¶„ ì¢‹ì€ ìˆœê°„ì„ ê¸°ì–µí•´ë‘ì„¸ìš”!'},
            'ìŠ¬í””': {'emoji': 'ğŸ˜¢', 'description': 'ìŠ¬í”ˆ ê°ì •ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë•Œë¡œëŠ” ìŠ¬í””ì„ ëŠë¼ëŠ” ê²ƒë„ ìì—°ìŠ¤ëŸ¬ìš´ ì¼ì´ì—ìš”.'},
            'ì¤‘ë¦½': {'emoji': 'ğŸ˜', 'description': 'í‰ì˜¨í•œ ê°ì • ìƒíƒœì…ë‹ˆë‹¤. ì°¨ë¶„íˆ í•˜ë£¨ë¥¼ ë˜ëŒì•„ë³´ì„¸ìš”.'},
            'ë†€ëŒ': {'emoji': 'ğŸ˜®', 'description': 'ë†€ë€ ê°ì •ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆìƒì¹˜ ëª»í•œ ì¼ì´ ìˆì—ˆë‚˜ìš”?'},
            'ê³µí¬': {'emoji': 'ğŸ˜¨', 'description': 'ë‘ë ¤ìš´ ê°ì •ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜¼ì í˜ë“¤ì–´í•˜ì§€ ë§ˆì„¸ìš”.'},
            'í˜ì˜¤': {'emoji': 'ğŸ˜«', 'description': 'ë¶ˆì¾Œí•œ ê°ì •ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ë¶„ ì „í™˜ì´ í•„ìš”í•´ ë³´ì—¬ìš”.'}
        }

        self.analyzer = EmotionAnalyzer()
        self.current_music_file = None
        self.setup_ui()

    def setup_ui(self):
        self.setWindowTitle('ê°ì • ì¼ê¸°ì¥')
        self.setStyleSheet("""
            QMainWindow {
                background-color: #f8f9fa;
            }
        """)
        # ìŠ¤íƒ ìœ„ì ¯ ì„¤ì •
        self.stack = QStackedWidget()
        self.setCentralWidget(self.stack)

        # ì›¹ìº  ìœ„ì ¯
        self.webcam = WebcamWidget()
        self.webcam.capture_button.clicked.connect(self.start_analysis)
        self.stack.addWidget(self.webcam)

        # ê²°ê³¼ ìœ„ì ¯
        self.result = ResultWidget(self.emotion_data)
        self.result.new_button.clicked.connect(self.new_analysis)
        self.result.ai_music_button.clicked.connect(self.play_ai_music)
        self.result.youtube_music_button.clicked.connect(self.play_youtube_music)
        self.stack.addWidget(self.result)

        self.setMinimumSize(800, 600)
        self.center_window()

    def center_window(self):
        frame = self.frameGeometry()
        center = QDesktopWidget().availableGeometry().center()
        frame.moveCenter(center)
        self.move(frame.topLeft())

    def start_analysis(self):
        self.webcam.stop_camera()  # ì›¹ìº  ì¤‘ì§€
        results = self.analyzer.analyze_emotions()

        if results:
            gender = 'ë‚¨ì„±' if results['gender'] == 'Male' else 'ì—¬ì„±'
            self.result.update_result(gender, results['age_group'], results['emotion'])
            self.stack.setCurrentWidget(self.result)
        else:
            self.webcam.start_camera()  # ë¶„ì„ ì‹¤íŒ¨ì‹œ ì›¹ìº  ì¬ì‹œì‘
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    def new_analysis(self):
        self.webcam.start_camera()  # ì›¹ìº  ì¬ì‹œì‘
        self.stack.setCurrentWidget(self.webcam)

    def play_ai_music(self):
        try:
            emotion = self.result.emotion_label.text().split(': ')[1]
            self.current_music_file = self.analyzer.create_music(emotion)
            print(f"ìŒì•… íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {self.current_music_file}")

            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(self.current_music_file):
                raise FileNotFoundError(f"MIDI íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.current_music_file}")

            print("ìŒì•… ì¬ìƒ ì‹œë„...")
            # ì‹œìŠ¤í…œ ê¸°ë³¸ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ì‹¤í–‰
            if sys.platform == 'win32':
                os.system(f'start wmplayer "{self.current_music_file}"')
            else:
                os.system(f'open "{self.current_music_file}"')

            # ë²„íŠ¼ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.result.ai_music_button.setEnabled(False)
            self.result.youtube_music_button.setEnabled(False)
            self.result.is_playing = True

        except Exception as e:
            print(f"ìŒì•… ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            QMessageBox.warning(self, "ì˜¤ë¥˜", f"ìŒì•… ì¬ìƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

            # ë²„íŠ¼ ìƒíƒœ ë³µêµ¬
            self.result.ai_music_button.setEnabled(True)
            self.result.youtube_music_button.setEnabled(True)
            self.result.is_playing = False

        except Exception as e:
            print(f"ìŒì•… ì¬ìƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            QMessageBox.warning(self, "ì˜¤ë¥˜", f"ìŒì•… ì¬ìƒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

            # ë²„íŠ¼ ìƒíƒœ ë³µêµ¬
            self.result.stop_button.setEnabled(False)
            self.result.ai_music_button.setEnabled(True)
            self.result.youtube_music_button.setEnabled(True)
            self.result.is_playing = False

    def stop_music(self):
        try:
            if hasattr(self, 'current_player'):
                if self.current_player == 'winsound':
                    import winsound
                    winsound.PlaySound(None, winsound.SND_PURGE)
                elif self.current_player == 'media_player':
                    if sys.platform == 'win32':
                        os.system('taskkill /F /IM wmplayer.exe')
        except Exception as e:
            print(f"ìŒì•… ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # ë²„íŠ¼ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.result.stop_button.setEnabled(False)
        self.result.ai_music_button.setEnabled(True)
        self.result.youtube_music_button.setEnabled(True)
        self.result.is_playing = False

    def play_youtube_music(self):
        current_emotion = self.result.emotion_label.text().split(': ')[1]
        recommendation = self.analyzer.get_music_recommendations(
            self.result.details_label.text().split('\n')[0].split(': ')[1],
            self.result.details_label.text().split('\n')[1].split(': ')[1],
            current_emotion
        )
        if recommendation:
            driver = self.analyzer.play_youtube_video(recommendation['url'])
            if driver:
                # í”¼ë“œë°± ëŒ€í™”ìƒì í‘œì‹œ
                feedback = QMessageBox.question(
                    self,
                    "ìŒì•… í”¼ë“œë°±",
                    "ìŒì•…ì´ ë§ˆìŒì— ë“œì…¨ë‚˜ìš”?",
                    QMessageBox.Yes | QMessageBox.No
                )
                # í”¼ë“œë°± ì €ì¥
                self.analyzer.update_weights(
                    current_emotion,
                    feedback == QMessageBox.Yes
                )
                driver.quit()

# ë©”ì¸ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    app = QApplication(sys.argv)
    window = EmotionAnalyzerGUI()
    window.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    main()